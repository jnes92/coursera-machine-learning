# Heroes of Deep Learning - Part 2 

## Week 1 - Yoshua Bengio

how did you started dl ?
- started as kid with science fiction
- started paper reading in 1985 
- exciting field 
- worked on recurrent nets, speech recogntion
- moved to MIT

thinking evolved overtime:
- starting with experiments
- understanding backprop why 
- early 2000s only intuition for powerful deep nets
- experiments did not work at the beginning

biggest surprise +/-
- mistake: think like everyone else, flat parts would derivatives nearly 0 so its "not possible"
- ReLu was working better than sigmoid
  

relation between dl - brain?
- information is distributed in many neurons
- instead of grand mother cell
- depth came later, early 2000s

most proud of - research group ideas :
- long term dependencies
- word embeddings
- initializing ideas
- devanishing gradients in deep nets
- importance of linear activation funcs
- unsupervised learnings: gans
- neural machine translation, attention
- can handle any kind of data structure
- sth different like backprop for neuroscience

relation dl - brain
- biological
- daydreaming
- puzzle of evidences from learning (brain)
- all ml concepts, ideas of globally training
- how brain can do sth like backprop
- general concept behind backprop? 
- larger family of this
- reinforement learning asks same question
- jeff hintons talk from 2007(?) 

unsupervised learning perspective:
- really important
- industry based on supervised
- so we need to label the data
- humans can do much more, explore new concepts
- 2-year old understand gravity, pressure, etc. without being told about it
- mental construction for how the world works by observation
- combining with reinforcement learning
- challenging, open field of possibilities

most exciting deeplearning:
- systems right now make mistakes
- not try to build systems to do useful
- how can a computer observe and learn to interact with the world
- basic research could be done by anyone
- system: how can i influence with my actions
- understanding high level abstractions
- as much as possible in a autonomous way

toy-problems, optimistic of tranfer to bigger problems
- transfers in a meta-way
- understand failures better
- reduce problem to things we can understand more easily
- research cycle is much faster

science of dl:
- fear: blind people try to find a way by luck
- stop and try to understand
- no need of math formulas, but logical understandment is needed
- research not to beat benchmark / company, more about answering important questions
- understand our algorithm better, what circumstances for which alg

tipps for beginners of ai
- different motivations / things
- researcher / engineer - different understandings needed
- **practice**, **practice**
- read a lot
- try to implement things by yourself
- dont just use frameworks
- experiment
- understand what you do, why other ppl do this or that
- book by bengio
- iclr proceeding papers
- dont be afraid of maths
