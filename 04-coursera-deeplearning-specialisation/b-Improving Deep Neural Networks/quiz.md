# DLS-2
## Week 1 - Quiz 1

1. 98,1,1
2. same distribution
3. 
   1. high bias -> increase units, layers
   2. high variance -> regularize, more train data
4. good training error, poor dev error -> high variance -> increase regularization, more data, bigger network
5. weight decay: A Regularization technique
6. lambda up -> weights down
7. you dont apply drouput, do not keep factor used in training
8. reduc regul, lower training error
9.  data augment, dropout, l2 reg
10. cost function faster